server:
  port: 8080

# The following setting allows overlapping bean definitions, because this demo project includes
# all three spring boot starters (LLM, Vector, and Hybrid) to demonstrate various routing strategies.
# In a realistic scenario, only one routing approach should be selected.
spring:
  main:
    allow-bean-definition-overriding: true
  jackson:
    deserialization:
      fail-on-unknown-properties: false

lmos:
  router:
    classifier:
      vector:
        enabled: true
      llm:
        enabled: true
      hybrid:
        enabled: true
    llm:
      provider: ${LLM_PROVIDER:azure_openai}
      api-key: ${LLM_API_KEY:your-api-key}
      base-url: ${LLM_BASE_URL:https://your-subdomain.openai.azure.com}
      model: ${LLM_MODEL_NAME:your-model}
      maxChatHistory: ${LLM_MAX_CHAT_HISTORY:10}
#      system-prompt: ${LLM_SYSTEM_PROMPT}
    embedding:
      store:
        host: ${EMBEDDING_STORE_HOST:localhost}
        port: ${EMBEDDING_STORE_PORT:6334}
      ranking:
        maxEmbeddings: ${EMBEDDING_RANKING_MAX_EMBEDDINGS:15}
        minWeight: ${EMBEDDING_RANKING_MIN_WEIGHT:5.0}
        minDistance: ${EMBEDDING_RANKING_MIN_DISTANCE:4.0}
        minMeanScore: ${EMBEDDING_RANKING_MIN_MEAN_SCORE:0.8}
        minRealDistance: ${EMBEDDING_RANKING_MIN_REAL_DISTANCE:0.3}
      model:
        provider: openai
        base-url: ${EMBEDDING_MODEL_OPENAI_BASE_URL:https://api.openai.com/v1/embeddings}
#        provider: huggingface
#        model-name: ${EMBEDDING_MODEL_HUGGINGFACE_MODEL_NAME:intfloat/multilingual-e5-large}
#        api-key: ${EMBEDDING_MODEL_HUGGINGFACE_API_KEY:your-api-key}
#        provider: local_onnx
#        model-path: ${EMBEDDING_MODEL_LOCAL_MODEL_PATH:/path/to/your/local/model.onnx}
#        tokenizer-path: ${EMBEDDING_MODEL_LOCAL_TOKENIZER_PATH:/path/to/your/local/tokenizer.json}
      enabled: ${EMBEDDING_ENABLED:true}
      document-path: ${EMBEDDING_MODEL_DOCUMENT_PATH:/path/to/your/capabilities/directory}


logging:
  level:
    sun:
      rmi: ERROR